{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leorm2002/Realized-volatility-ML/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9TVmDkXARTP"
      },
      "source": [
        "## Introduzione del problema\n",
        "\n",
        "# Introduzione\n",
        "Il seguente progetto è basato sulla challenge kaggle [Optiver Realized Volatility Prediction](https://www.kaggle.com/competitions/optiver-realized-volatility-prediction)\n",
        "\n",
        "La volatilità dei mercati finanziari rappresenta una misura fondamentale del rischio e dell'incertezza associati ai prezzi degli asset.\n",
        "\n",
        "La volatilità realizzata è una misura della variabilità dei rendimenti di un asset nel tempo. Più specificamente, essa viene calcolata come la deviazione standard dei rendimenti di un asset durante un determinato periodo di tempo.\n",
        "\n",
        "\n",
        "L'obiettivo è sviluppare un modello che in grado di stimare la volatilità realizzata per diverse azioni con un orizzonte temporale di 10 minuti, utilizzando *order book* e *trade log* dei precedenti 10 minuti.\n",
        "Il modello inoltre dovrà predire solo risultati sulle azioni usate in fase di training\n",
        "\n",
        "La challenge fornisce un dataset di training e prevede la submission del notebook per la valutazione con un dataset \"nascosto\" (con la garanzia di mantenere le stesse azioni utilizzate nel training, questo ci permette quindi di utilizzarle come parametro per il training). Non avendolo a disposizione si opterà per dividere il training set \"originale\" in un training set e un validation set, questo è possibile anche grazie alla grande quantità di dati disponibili nel dataset originale.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "dqxcWU37V2yi",
        "outputId": "7d2dec8b-c190-419d-ebac-5dbe59fea2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ngrok: command not found\n",
            "Collecting swifter[notebook]\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter[notebook]) (2.1.4)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter[notebook]) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter[notebook]) (2024.7.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from swifter[notebook]) (4.66.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter[notebook]) (7.7.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (8.4.0)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2.10.0->swifter[notebook])\n",
            "  Downloading dask_expr-1.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (3.6.8)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.0.0->swifter[notebook]) (3.0.13)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[notebook]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[notebook]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[notebook]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[notebook]) (2024.1)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter[notebook]) (14.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (3.20.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter[notebook]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter[notebook]) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook])\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (4.9.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[notebook]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter[notebook]) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter[notebook]) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->swifter[notebook]) (1.2.2)\n",
            "Downloading dask_expr-1.1.9-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16506 sha256=3130316835a441f315c97ee1a68e8bd5ee43b6d602eb7219e82aeb8c7f7eb715\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
            "Successfully built swifter\n",
            "Installing collected packages: jedi, dask-expr, swifter\n",
            "Successfully installed dask-expr-1.1.9 jedi-0.19.1 swifter-1.4.0\n",
            "Requirement already satisfied: swifter[groupby] in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter[groupby]) (2.1.4)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter[groupby]) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter[groupby]) (2024.7.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from swifter[groupby]) (4.66.5)\n",
            "Collecting ray>=1.0.0 (from swifter[groupby])\n",
            "  Downloading ray-2.35.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (24.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (8.4.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter[groupby]) (1.1.9)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[groupby]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[groupby]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[groupby]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter[groupby]) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray>=1.0.0->swifter[groupby]) (2.32.3)\n",
            "Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter[groupby]) (14.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (3.20.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter[groupby]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter[groupby]) (1.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=1.0.0->swifter[groupby]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=1.0.0->swifter[groupby]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=1.0.0->swifter[groupby]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=1.0.0->swifter[groupby]) (0.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0.0->swifter[groupby]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0.0->swifter[groupby]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0.0->swifter[groupby]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray>=1.0.0->swifter[groupby]) (2024.7.4)\n",
            "Downloading ray-2.35.0-cp310-cp310-manylinux2014_x86_64.whl (65.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.35.0\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.112.2 h11-0.14.0 pyngrok-7.2.0 starlette-0.38.2 uvicorn-0.30.6\n"
          ]
        }
      ],
      "source": [
        "should_install = False\n",
        "should_install = True\n",
        "if should_install:\n",
        "  !pip install swifter[notebook]\n",
        "  !pip install -U swifter[groupby]\n",
        "  !pip install lightgbm\n",
        "  !pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mXaye5bCcbRf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from lightgbm import LGBMRegressor\n",
        "import swifter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "import gdown\n",
        "import pickle\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fZudLbjNeU6d"
      },
      "outputs": [],
      "source": [
        "TARGET_PATH = 'data/train.csv'\n",
        "BOOK_PATH = 'data/book_train.parquet'\n",
        "TRADE_PATH = 'data/trade_train.parquet'\n",
        "DATA_URL = \"1oRxakzOU7iAL4SFJYffAZuKF3oHSOuDX\"\n",
        "DATA_URL = f\"https://drive.google.com/uc?id={DATA_URL}\"\n",
        "\n",
        "MODEL_URL = \"1mBoH9K3HfImIfc13nEM0fKCbdo12uBU3\"\n",
        "MODEL_URL = f\"https://drive.google.com/uc?id={MODEL_URL}\"\n",
        "\n",
        "DOWLOADED_FILE = \"data.zip\"\n",
        "DOWNLOADED_MODEL_FILE = \"models.zip\"\n",
        "\n",
        "# Modalita di funzionamento\n",
        "\n",
        "# 0 carica dataset, preprocess, salvataggio dati preprocessati, addestramento, valutazione, salvataggio modelli\n",
        "# 1 carica dati preprocessati addestramento, valutazione, salvataggio modelli\n",
        "# 2 carica modelli preaddestrati, esponi endpoint per inferenza\n",
        "\n",
        "MODE = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mode(line, cell=None):\n",
        "    '''Skips execution of the current line/cell if line evaluates to True.'''\n",
        "    modes = [int(x) for x in (line.strip().split(\",\"))]\n",
        "    if MODE not in modes:\n",
        "        return\n",
        "\n",
        "    get_ipython().ex(cell)\n",
        "\n",
        "def load_ipython_extension(shell):\n",
        "    '''Registers the skip magic when the extension loads.'''\n",
        "    shell.register_magic_function(mode, 'line_cell')\n",
        "\n",
        "def unload_ipython_extension(shell):\n",
        "    '''Unregisters the skip magic when the extension unloads.'''\n",
        "    del shell.magics_manager.magics['cell']['skip']\n",
        "\n",
        "\n",
        "load_ipython_extension(get_ipython())"
      ],
      "metadata": {
        "id": "MTZzVs-oWKEv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMYzS_6gcbRh"
      },
      "source": [
        "## Caricamento dati"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoqIiLVpcbRi"
      },
      "source": [
        "Il dataset è fornito separato in più file. Uno contente il valore della variabile target (volatilita realizzata a 10 minuti),l'id per ricondursi allo strumento finanziario e al momento preciso.\n",
        "Un insieme di files, divisi per strumento finaziario contenenti 10 minuti di order book e trade effettuati"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "import shutil\n",
        "\n",
        "# scarichiamo\n",
        "if not os.path.exists(DOWLOADED_FILE):\n",
        "    print(f\"File does not exist. Downloading...\")\n",
        "    gdown.download(DATA_URL, DOWLOADED_FILE, quiet=True)\n",
        "else:\n",
        "    print(f\"File already exists\")\n"
      ],
      "metadata": {
        "id": "au3bvcjKJDOM",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "target_folder = DOWLOADED_FILE.replace(\".zip\", \"\")\n",
        "if os.path.exists(target_folder):\n",
        "    shutil.rmtree(target_folder)\n",
        "shutil.unpack_archive(DOWLOADED_FILE)\n"
      ],
      "metadata": {
        "id": "En4ItAF7ZwpX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ybjHB0tGcbRj"
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "target = pd.read_csv(TARGET_PATH)\n",
        "#creiamo un id univoco che rappresenti il valore target di una determinata azione in un determinato momento\n",
        "target['row_id'] = target['stock_id'].astype(str) + '-' + target['time_id'].astype(str)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yx4_PIk4aCMc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "09cy01pwcbRk"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JUun-ArcbRk"
      },
      "source": [
        "Vediamo la struttura dell'order book e dei trade relativi alla stock con id = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "63EstGAxcbRl"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "trade_example =  pd.read_parquet(TRADE_PATH + '/stock_id=0')\n",
        "book_example = pd.read_parquet(BOOK_PATH + '/stock_id=0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4SwGcNVip1k"
      },
      "source": [
        "Significato delle colonne:\n",
        "\n",
        "\n",
        "*   time_id: codice identificativo per il bucket temporale, divide in quanti di tempo da 10 minuti\n",
        "*   seconds_in_bucket: secondi dall'inizio del bucket [0-600[\n",
        "*   bid_price_1 buy price più competitivo\n",
        "*   ask_price_1 sell price più competitivo\n",
        "*   bid_price_2 secondo buy price più competitivo\n",
        "*   ask_price_2 secondo sell price più competitivo\n",
        "*   bid_size_1 numero di azioni per bid_price_1\n",
        "*   ask_size_1 numero di azioni ask_price_1\n",
        "*   bid_size_2 numero di azioni bid_price_2\n",
        "*   ask_size_2 numero di azioni ask_price_2\n",
        "\n",
        "Esempio, una riga con time_id = 2 e seconds_in_bucket=300 rappresenta il momentod dopo 25 minuti dall'inizio del primo interallo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y_sAcPx-cbRm"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "book_example.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aOXgB-Rp9McO"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "numero_tempi = len(book_example[\"time_id\"].unique())\n",
        "print(\"Numero di intevalli temporali:\", numero_tempi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UY0rtU0jB2E"
      },
      "source": [
        "Significato delle colonne:\n",
        "*   time_id codice identificativo per il bucket temporale\n",
        "*   seconds_in_bucket secondi dall'inizio del bucket (0 based)\n",
        "*   price: prezzo a cui è avvenuto lo scambio\n",
        "*   size: numero di azioni scambiate\n",
        "*   oder_count: numero di ordini\n",
        "Le prime due colonne referenziano un particolare order book"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "astfbIoIcbRn"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "trade_example.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-B1byF2cbRo"
      },
      "source": [
        "Procediamo a quindi a caricare tuatti i dati in due data frame, l'informazione relativa all'azione relativa al book non è presente nei dati ma dipende dalla cartella, scriviamo quindi una funzione che ci permetta di arricchire i dati caricati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aP4ISZglcbRp"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "def load_and_enrich(folder : str, sub_folder : str, stock_range: list):\n",
        "    path = os.path.join(folder, sub_folder)\n",
        "    stock_id = sub_folder.split('=')[-1]\n",
        "    stock_id = int(stock_id)\n",
        "    if stock_range and stock_id not in stock_range:\n",
        "        return None\n",
        "    try:\n",
        "        parquet_file = os.listdir(path)[0]\n",
        "        parquet_file = os.path.join(path, parquet_file)\n",
        "        df = pd.read_parquet(parquet_file)\n",
        "        df.loc[:,'stock_id'] = stock_id\n",
        "        return df\n",
        "    except:\n",
        "      print(sub_folder)\n",
        "\n",
        "#carica, per ogni sotto cartella contenuta nella cartella passata come parametro il file parquet contenuto\n",
        "def load_folder(folder: str, max_num : int = None):\n",
        "    folders = os.listdir(folder)\n",
        "    stock_range = None\n",
        "    df = pd.DataFrame()\n",
        "    dfs = []\n",
        "    if max_num:\n",
        "      stock_range = [x for x in range(max_num)]\n",
        "    for subfolder in folders:\n",
        "      out = load_and_enrich(folder, subfolder, stock_range)\n",
        "      if out is None:\n",
        "        continue\n",
        "      dfs.append(out)\n",
        "      #df = pd.concat([df, out])\n",
        "\n",
        "    return pd.concat(dfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHAworYOhfYL"
      },
      "source": [
        "A questo punto carichiamo l'order book della prima azione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "1D4zdYHUf0qF"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "books = load_folder(BOOK_PATH, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e9SmKxd7uq3c"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "orders = load_folder(TRADE_PATH, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GBdHN83k_dx"
      },
      "source": [
        "## Analisi iniziale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8LBRN--ahV1D"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "books"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A-Sm6UGRu7t4"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "orders.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HTRnJHLPWL-f"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "lunghezza_serie = books.groupby('stock_id')['time_id'].nunique().unique()\n",
        "print(\"Serie temporali per ogni azione:\", lunghezza_serie)\n",
        "# vediamo come ogni serie abbia la stessa lunghezza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uPLvye2uYiB"
      },
      "source": [
        "Vediamo ora un l'order book per una singola azione per un singolo bucket e le informazioni relative allo scambio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2bNkV0N_uiUg"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "stock = 0\n",
        "time = 5\n",
        "key = \"seconds_in_bucket\"\n",
        "order_book = books[(books['stock_id'] == stock) & (books['time_id'] == time)]\n",
        "trade_log = orders[(orders['stock_id'] == stock) & (orders['time_id'] == time)]\n",
        "samples = [\"bid_price1\", \"ask_price1\", \"bid_price2\", \"ask_price2\"]\n",
        "# plottiamo il valore delle serie\n",
        "for sample in samples:\n",
        "    plt.plot(order_book[key],order_book[sample], label=sample)\n",
        "plt.plot(trade_log[key],trade_log[\"price\"], label=\"price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "trade_log.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CpBffwFwyq2"
      },
      "source": [
        "Confrontiamo ora il volume degli scambi contro il volume degli ordini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wrZ2PDeUw4M2"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "to_plot = books.merge(orders, on=[\"time_id\", \"seconds_in_bucket\", \"stock_id\"], how=\"left\")\n",
        "to_plot[\"trade_ratio\"] = to_plot[\"size\"] * 2 / (to_plot[\"ask_size1\"] + to_plot[\"bid_size1\"] + to_plot[\"bid_size2\"] + to_plot[\"ask_size2\"])\n",
        "to_plot[\"trade_ratio\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUemuM6Mr-yD"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "L'obbiettivo del modello è predire la Realized Volatility di un azione con una finestra temporale di 10 minuti. La Realized Volatility σ è definita come la radice quadrata della somma dei quadrati del logaritmo dei rendimenti ovvero:\n",
        "  \n",
        "$$\n",
        "\\sigma = \\sqrt{\\sum_{t}{\\log(\\frac{S_{t-1}}{S_{t}})^2}}\n",
        "$$\n",
        "Dove $S_t$ è il valore del prodotto all'istante t.\n",
        "Usiamo WAP (weighted average price) per calcolare il prezzo di uno strumento all'istante $t$ secondo la formula:\n",
        "$$\n",
        "WAP = \\frac\n",
        "{BidPrice_1 \\cdot AskSize_1 + AskPrice_1 \\cdot BidSize_1}\n",
        "{BidSize_1 + AskSize_1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhKFGWdJsGwX"
      },
      "source": [
        "\n",
        "Iniziamo a studiare ed introdurre indicatori che ci permettano di sintetizzare le informazioni presenti in ogni order book a ogni secondo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "irPSV-0hsxVk"
      },
      "outputs": [],
      "source": [
        "%%mode 0,2\n",
        "def log_return(list_stock_prices, column=\"wap\"):\n",
        "    log_returns = np.log(list_stock_prices[column]).diff()\n",
        "    col = column + \"_log_return\"\n",
        "    list_stock_prices[col] = log_returns\n",
        "    return list_stock_prices\n",
        "\n",
        "def realized_volatility(series_log_return):\n",
        "    return np.sqrt(np.sum(series_log_return**2))\n",
        "\n",
        "def wap1(df):\n",
        "    return (df['bid_price1'] * df['ask_size1']+df['ask_price1'] * df['bid_size1'])  / (df['bid_size1']+ df['ask_size1'])\n",
        "\n",
        "def wap2(df):\n",
        "    return (df['bid_price2'] * df['ask_size2']+df['ask_price2'] * df['bid_size2'])  / (df['bid_size2']+ df['ask_size2'])\n",
        "\n",
        "def wap_balance(df):\n",
        "    return abs(df[\"wap1\"] - df[\"wap2\"])\n",
        "\n",
        "def relativeBidAskSpread1(df):\n",
        "    # Da un indicazione della liquidita dell'asset relativa\n",
        "    return (df[\"ask_price1\"] - df[\"bid_price1\"]) / ((df[\"ask_price1\"] + df[\"bid_price1\"])/2)\n",
        "\n",
        "def relativeBidAskSpread2(df):\n",
        "    # Da un indicazione della liquidita dell'asset relativa\n",
        "    return (df[\"ask_price2\"] - df[\"bid_price2\"]) / ((df[\"ask_price2\"] + df[\"bid_price2\"])/2)\n",
        "\n",
        "def orderImbalance(df):\n",
        "    # Da un indicazione della liquidita dell'asset in termini di quantita\n",
        "    return (df[\"bid_size1\"] - df[\"ask_size1\"]) / (df[\"bid_size1\"] + df[\"ask_size1\"])\n",
        "\n",
        "def volume(df):\n",
        "    return df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"]\n",
        "\n",
        "def volume_imbalance(df):\n",
        "    return abs((df[\"bid_size1\"] + df[\"bid_size2\"]) - (df[\"ask_size1\"] + df[\"ask_size2\"]))\n",
        "\n",
        "def bid_ask_spread(df):\n",
        "    return abs(df['bid_price1'] - df['bid_price2'] - (df['ask_price1'] - df['ask_price2']))\n",
        "\n",
        "def log_return_wap1(list_stock_prices):\n",
        "    return log_return(list_stock_prices, column=\"wap1\")\n",
        "\n",
        "def log_return_wap2(list_stock_prices):\n",
        "    return log_return(list_stock_prices, column=\"wap2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHxwQQrr4fI7"
      },
      "source": [
        "Creiamo ora le procedure che ci permettono di sintetizzare le feature in un singolo record rappresentante il bucket su cui faremo la previsione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oXsZfZxk4T2m"
      },
      "outputs": [],
      "source": [
        "%%mode 0,2\n",
        "#definiamo le operazioni da effettuare su ogni feture per aggregare\n",
        "feature_operations_dict = {\n",
        "        'wap1': [np.sum, np.std],\n",
        "        'wap2': [np.sum, np.std],\n",
        "        'wap1_log_return': [realized_volatility],\n",
        "        'wap2_log_return': [realized_volatility],\n",
        "        'relativeBidAskSpread1':[np.sum, np.max],\n",
        "        'relativeBidAskSpread2':[np.sum, np.max],\n",
        "        'order_imbalance':[np.sum, np.max],\n",
        "        'volume':[np.sum, np.max],\n",
        "        'volume_imbalance':[np.sum, np.max],\n",
        "        \"bid_ask_spread\":[np.sum,  np.max],\n",
        "}\n",
        "\n",
        "feature_operations_time_dict = {\n",
        "    'wap1_log_return': [realized_volatility],\n",
        "    'wap2_log_return': [realized_volatility]\n",
        "}\n",
        "\n",
        "feature_operations_log_dict = {\n",
        "      'price_log_return':[realized_volatility],\n",
        "      'size':[np.sum, np.max, np.min],\n",
        "      'order_count':[np.sum,np.max],\n",
        "      'volume':[np.sum,np.max,np.min]\n",
        "}\n",
        "\n",
        "feature_operations_log_time_dict = {\n",
        "    'price_log_return':[realized_volatility],\n",
        "    'size':[np.sum],\n",
        "    'order_count':[np.sum],\n",
        "  }\n",
        "\n",
        "windows = [150, 300, 450]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rHjE5bMd_Esk"
      },
      "outputs": [],
      "source": [
        "%%mode 0,2\n",
        "def apply_window(bucket, window, operations, add_suffix : bool = True):\n",
        "    df_feature = bucket[bucket['seconds_in_bucket'] >= window].groupby(['time_id']).agg(operations).reset_index()\n",
        "    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "    # Add a suffix to differentiate windows\n",
        "    if add_suffix:\n",
        "        df_feature = df_feature.add_suffix('_' + str(window))\n",
        "\n",
        "    return df_feature\n",
        "\n",
        "def apply_windows(bucket, windows, operationsAll, operationsSub):\n",
        "    df = apply_window(bucket, 0, operationsAll, False)\n",
        "    for window in windows:\n",
        "        df_feature = apply_window(bucket, window, operationsSub)\n",
        "        key = 'time_id__' + str(window)\n",
        "        df = df.merge(df_feature, how='left', left_on='time_id_', right_on=key)\n",
        "        df.drop(key, axis=1, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V1fowNXm2Frd"
      },
      "outputs": [],
      "source": [
        "%%mode 0,2\n",
        "def add_features_to_book(book):\n",
        "    book['wap1'] = wap1(book)\n",
        "    book['wap2'] = wap2(book)\n",
        "\n",
        "    # log return wap1\n",
        "    book = book.swifter.groupby(['time_id']).apply(log_return_wap1).reset_index(drop=True)\n",
        "    book = book[~book['wap1_log_return'].isnull()] # rimuoviamo t0\n",
        "\n",
        "    # log return wap2\n",
        "    book = book.swifter.groupby(['time_id']).apply(log_return_wap2).reset_index(drop=True)\n",
        "    book = book[~book['wap2_log_return'].isnull()] # rimuoviamo t0\n",
        "\n",
        "    book['wap_diff'] = wap_balance(book)\n",
        "    book[\"relativeBidAskSpread1\"] = relativeBidAskSpread1(book)\n",
        "    book[\"relativeBidAskSpread2\"] = relativeBidAskSpread2(book)\n",
        "    book[\"order_imbalance\"] = orderImbalance(book)\n",
        "    book[\"volume\"] = volume(book)\n",
        "    book[\"volume_imbalance\"] = volume_imbalance(book)\n",
        "    book[\"bid_ask_spread\"] = bid_ask_spread(book)\n",
        "    return book\n",
        "\n",
        "\n",
        "def add_features_to_log(log):\n",
        "    log = log.swifter.groupby([\"time_id\"]).apply(lambda x: log_return(x, column=\"price\")).reset_index(drop=True)\n",
        "    log = log[~log['price_log_return'].isnull()] # rimuoviamo t0\n",
        "    log[\"volume\"] = log[\"price\"] + log[\"price\"]\n",
        "\n",
        "    return log\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,2\n",
        "def process(books, logs):\n",
        "    books = add_features_to_book(books)\n",
        "    books = apply_windows(books, windows, feature_operations_dict, feature_operations_time_dict)\n",
        "\n",
        "    logs = add_features_to_log(logs)\n",
        "    logs = apply_windows(logs, windows, feature_operations_log_dict, feature_operations_log_time_dict)\n",
        "    # merge logs on books\n",
        "    ret = books.merge(logs, on=\"time_id_\", how=\"left\")\n",
        "    return ret\n"
      ],
      "metadata": {
        "id": "A_9Dz486mth3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW50Pl-X5jXN"
      },
      "source": [
        "Applichiamo quindi ai nostri order book le trasformazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5acWumI9eUHk"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "def load_and_enrich(folder : str, trade_logs_folder : str, sub_folder : str, stock_range: list):\n",
        "    books_path = os.path.join(folder, sub_folder)\n",
        "    trades_path = os.path.join(trade_logs_folder, sub_folder)\n",
        "\n",
        "    stock_id = sub_folder.split('=')[-1]\n",
        "    stock_id = int(stock_id)\n",
        "    if stock_range and stock_id not in stock_range:\n",
        "        return None\n",
        "    try:\n",
        "        books_parquet_file = os.listdir(books_path)[0]\n",
        "        books_parquet_file = os.path.join(books_path, books_parquet_file)\n",
        "\n",
        "        trades_parquet_file = os.listdir(trades_path)[0]\n",
        "        trades_parquet_file = os.path.join(trades_path, trades_parquet_file)\n",
        "\n",
        "        ret = process(pd.read_parquet(books_parquet_file), pd.read_parquet(trades_parquet_file))\n",
        "        ret['stock_id'] = stock_id\n",
        "        print(\"Elaborated \", stock_id)\n",
        "        return ret\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "#carica, per ogni sotto cartella contenuta nella cartella passata come parametro il file parquet contenuto\n",
        "def load_folder(order_books_folder: str, trade_logs_folder: str, max_num : int = None):\n",
        "    folders = os.listdir(order_books_folder)\n",
        "    stock_range = None\n",
        "    df = pd.DataFrame()\n",
        "    dfs = []\n",
        "    if max_num:\n",
        "      stock_range = [x for x in range(max_num)]\n",
        "    i = 0\n",
        "    for subfolder in folders:\n",
        "      out = load_and_enrich(order_books_folder, trade_logs_folder, subfolder, stock_range)\n",
        "      if out is None:\n",
        "        continue\n",
        "      dfs.append(out)\n",
        "      i += 1\n",
        "      print(i, \"/\", max_num)\n",
        "\n",
        "    return pd.concat(dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "def save(df, name):\n",
        "  df.to_parquet(name)\n",
        "def load(name):\n",
        "  df = pd.read_parquet(name)\n",
        "  return df\n",
        "\n",
        "name = \"df20.parquet\"\n"
      ],
      "metadata": {
        "id": "iT3lIak7-eOZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OQVpLDgJIL6G",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "df = load_folder(BOOK_PATH, TRADE_PATH)\n",
        "save(df, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTbSsQyZsrfa"
      },
      "source": [
        "## Analisi delle feature introdotte e valutazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nqX_utShiZHW"
      },
      "outputs": [],
      "source": [
        "%%mode 0\n",
        "print(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo come alcune righe hanno feature mancanti (questo è derivato dalla mancanza di scambi in parte dei bucket temporali) rimpiazziamo questi NaN con zero visto che tutte le misure interessate ha senso siano 0 in caso di mancanza di dati"
      ],
      "metadata": {
        "id": "BT7ZpIRBf7se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "O_Ef_pIsgZek"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0\n",
        "print(df.isna().sum())"
      ],
      "metadata": {
        "id": "ZQO6BLcOghSL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 1\n",
        "df = load(name)"
      ],
      "metadata": {
        "id": "9j7F4fqvXVVu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOnE8ePEFDKP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "correlation_matrix = df.corr()"
      ],
      "metadata": {
        "id": "AU5FmDnzFMZy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "plt.figure(figsize=(20, 20))  # Adjust the figure size (width, height)\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', annot_kws={\"size\": 8})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dho-ut8_9s2B"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "threshold = 0.8\n",
        "high_corr = correlation_matrix[(correlation_matrix.abs() > threshold) & (correlation_matrix.abs() < 1.0)]\n",
        "\n",
        "# Melt the correlation matrix to get feature pairs\n",
        "high_corr_pairs = high_corr.unstack().dropna().reset_index()\n",
        "high_corr_pairs.columns = ['Metrica 1', 'Metrica 2', 'Correlation']\n",
        "high_corr_pairs = high_corr_pairs.sort_values(by='Correlation', ascending=False)\n",
        "# filtriamo le feature che sono logaritmo dei ritorni in ientrambi gli indicatori\n",
        "def is_log_ret_feature(df):\n",
        "  return df[~df['Metrica 1'].str.contains('log_return') & ~df['Metrica 2'].str.contains('log_return')]\n",
        "\n",
        "high_corr_pairs = is_log_ret_feature(high_corr_pairs)\n",
        "\n",
        "def extract_key(s):\n",
        "    return \"_\".join(s.split(\"_\")[:-1]) if \"_\" in s else s\n",
        "\n",
        "def are_key_equal(df):\n",
        "  return df[df['Metrica 1'].apply(extract_key) != df['Metrica 2'].apply(extract_key)]\n",
        "\n",
        "def filter_mobile_series(df):\n",
        "  patterns_to_filter = [\"150\", \"300\", \"450\"]\n",
        "\n",
        "  # Filtro per escludere righe in cui qualsiasi metrica contiene uno dei pattern\n",
        "  filtered_df = df[~df['Metrica 1'].str.contains('|'.join(patterns_to_filter), na=False) &\n",
        "                  ~df['Metrica 2'].str.contains('|'.join(patterns_to_filter), na=False)]\n",
        "  return filtered_df\n",
        "\n",
        "high_corr_pairs = are_key_equal(high_corr_pairs)\n",
        "high_corr_pairs = filter_mobile_series(high_corr_pairs)\n",
        "print(\"Features con rischio di collinearita  (|corr| > 0.8):\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(30, 10))\n",
        "\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create the table\n",
        "table = ax.table(cellText=high_corr_pairs.values, colLabels=high_corr_pairs.columns, cellLoc='center', loc='center')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EZtK9N83GO2J"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo molte correlazioni, anche filtrando per andare ad eliminare tutto ciò che deriva da medie mobili (che sono autocorrelate tra di loro e con la serie completa).\n",
        "Alcune sono prevedibili come la correlazione tra tutte le measure riferite al primo o al secondo prezzo.\n",
        "Notiamo come sia presente una correlazione tra il numero di ordini e il numero di azioni ordinate e il volume in valuta.\n",
        "E' presente inoltre una correlazione tra tutte le measure relative e le loro controparti assolute"
      ],
      "metadata": {
        "id": "pAliIgYQKeEn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJHBN2CNsfn7"
      },
      "source": [
        "## Divisione del data-set in training e test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "APwdEFvTsqbV"
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "#creiamo l'id per ricondurci alle previsioni\n",
        "df[\"row_id\"] = df[\"stock_id\"].astype(str) + \"-\" + df[\"time_id_\"].astype(str)\n",
        "#prendiamo le previsioni relative solo alle azioni e ai bucket temporali che abbiamo ottenuto\n",
        "y = df.merge(target, on=\"row_id\", how=\"left\")[\"target\"]\n",
        "x = df.drop(columns=[\"time_id_\", \"row_id\"], inplace=False)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=3107, test_size = 0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni di utilita comuni"
      ],
      "metadata": {
        "id": "7VgtOF0_4E_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
        "\n",
        "def evaluate(predictions, target):\n",
        "  R2 = round(r2_score(y_true = target, y_pred = predictions),3)\n",
        "  RMSPE = round(rmspe(y_true = target, y_pred = predictions),3)\n",
        "  print(f'Performance: R2 score: {R2}, RMSPE: {RMSPE}')\n",
        "  return R2, RMSPE\n",
        "\n",
        "def plot_feature_importance(model):\n",
        "    print(\"Elenco delle feature valutate dal modello in ordine di importanza\")\n",
        "    # Feature Importances\n",
        "    importances = model[\"regressor\"].best_estimator_.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    features = model[\"preprocessor\"].get_feature_names_out()\n",
        "\n",
        "    # Plotting the feature importances\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.title(\"Feature Importances\")\n",
        "    plt.barh(range(len(features)), importances[indices], align=\"center\")\n",
        "    plt.yticks(range(len(features)), features[indices])\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions_vs_real(predictedd, reall, n):\n",
        "    print(f\"Elenco dei primi {n} valori della serie predetta contro i valori reali\")\n",
        "    predicted = predictedd[:n]\n",
        "    real = reall[:n]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plotting the real data as points\n",
        "    plt.scatter(range(len(real)), real, label=\"Real\", color=\"blue\", marker='o')\n",
        "\n",
        "    # Plotting the predicted data as points\n",
        "    plt.scatter(range(len(predicted)), predicted, label=\"Predicted\", color=\"red\", marker='x')\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel(\"Data Points\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.title(\"Predicted vs Real Data Points\")\n",
        "\n",
        "    # Adding a legend to differentiate between predicted and real data\n",
        "    plt.legend()\n",
        "\n",
        "    # Adding a grid for better readability\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(pipeline, y, target, evaluate_importance = True):\n",
        "  predictions = pipeline.predict(y)\n",
        "  evaluate(predictions, target)\n",
        "  if evaluate_importance:\n",
        "      print()\n",
        "      plot_feature_importance(pipeline)\n",
        "  print()\n",
        "  plot_predictions_vs_real(predictions, target, 50)"
      ],
      "metadata": {
        "id": "hCRDsc-2xq52"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "def get_preprocessing_pipeline(grid_search, one_hot_encode_stocks = False):\n",
        "    # one hot encoding sull'id dell'azione\n",
        "    transformer = [(\"stock_id\", OneHotEncoder(handle_unknown=\"ignore\"), [\"stock_id\"])] if one_hot_encode_stocks else []\n",
        "\n",
        "    return Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", ColumnTransformer( transformer, remainder='passthrough')),\n",
        "        #('scaler', StandardScaler()),\n",
        "        (\"regressor\", grid_search)\n",
        "    ])\n",
        "\n",
        "def my_scorer(model, x, y):\n",
        "  y_pred = model.predict(x)\n",
        "  return rmspe(y, y_pred)\n"
      ],
      "metadata": {
        "id": "HYmfoPmMz3qA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPC6D9G0rjSO"
      },
      "source": [
        "## Approccio naive (idea presa da esempio kaggle)\n",
        "La volatilità tende ad essere autocorrelata, possiamo creare un modello base che predica la correlazione stessa dei dati passati in input, sarà un benchmarck iniziale per valutare l'accuratezza di modelli che andremo a produrre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vN-WeA1MCFwH"
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "\n",
        "def realized_volatility_naive(df_book_data):\n",
        "    #raggruppo applico la funzione sopra a ogni stock\n",
        "    return df_book_data['wap1_log_return_realized_volatility']\n",
        "\n",
        "class NaiveModel:\n",
        "    def predict(self, X):\n",
        "        #return realized_volatility as an array\n",
        "\n",
        "        return realized_volatility_naive(X).to_numpy()\n",
        "def get_naive_model():\n",
        "    return NaiveModel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CSVG8bRXrhxC"
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "naive_model = get_naive_model()\n",
        "evaluate_model(naive_model, x_test, y_test, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUcbpEmbt7Sa"
      },
      "source": [
        "## Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "from sklearn.linear_model import Lasso\n",
        "values = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
        "\n",
        "lasso = GridSearchCV(estimator=Lasso(), cv=2, param_grid=dict(alpha=values), scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n",
        "lasso = get_preprocessing_pipeline(lasso)\n",
        "lasso.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "RYRi6UvusAjg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "lasso.predict(x_test)"
      ],
      "metadata": {
        "id": "pkhNhx31klam"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "evaluate_model(lasso, x_test, y_test, False)"
      ],
      "metadata": {
        "id": "hipMndn0sZ6h"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbbCfOjqFDQ"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "def get_random_forest(params, x, y):\n",
        "  rf = RandomForestRegressor(random_state=3107, max_features=\"sqrt\", n_jobs=-1, verbose = 0)\n",
        "  gs = GridSearchCV(estimator=rf, cv=2, param_grid=params, scoring='neg_mean_squared_error',n_jobs=1, verbose=10)\n",
        "  gs = get_preprocessing_pipeline(gs)\n",
        "  gs.fit(x, y)\n",
        "  return gs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rwudVN9VuF4l"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "no_params_rf = get_random_forest({}, x_train, y_train)"
      ],
      "metadata": {
        "id": "gg_VL78Svhv2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPQVjwYid5MB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "fyqW4HwJS7QM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "evaluate_model(no_params_rf, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300, 400],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "rf_gs = get_random_forest(param_grid, x_train, y_train)"
      ],
      "metadata": {
        "id": "g9jCrKPjvp0b"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Co1mRFjqHSt"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "roAd0SWxqY_L"
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "from xgboost import XGBRegressor\n",
        "xgbm = XGBRegressor(objective='reg:squarederror', reg_alpha=0.0, reg_lambda=1.0, n_estimators=200, verbose_eval=True)\n",
        "xgbm = get_preprocessing_pipeline(xgbm)\n",
        "xgbm.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "evaluate_model(xgbm, x_test, y_test, False)"
      ],
      "metadata": {
        "id": "sOHioxCj0bMK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzcooyAOqLKA"
      },
      "source": [
        "## LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "4NwQNQzQqINx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%mode 0,1\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "def get_model(params, x ,y):\n",
        "    gs = RandomizedSearchCV(\n",
        "            estimator=LGBMRegressor(random_state=3107, n_jobs=1, verbose = 0),\n",
        "            param_distributions=params,\n",
        "            cv=3,\n",
        "            n_iter=40,\n",
        "            scoring='neg_root_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            random_state=3107,\n",
        "            refit=True,\n",
        "            verbose = 10) # necessaria da documentazione quando utilizziamo la grid search all'interno di una\n",
        "    gs = get_preprocessing_pipeline(gs)\n",
        "    gs.fit(x, y)\n",
        "    return gs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "lgbm_no_params = get_model({}, x_train, y_train)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Uq_AvgzpxBgh"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "evaluate_model(lgbm_no_params, x_test, y_test)#Performance: R2 score: 0.811, RMSPE: 0.26"
      ],
      "metadata": {
        "id": "aDl1-mg2xb5X"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "param_grid = {\n",
        "    'num_leaves': sp_randint(20, 150),\n",
        "    'max_depth': sp_randint(-1, 50),\n",
        "    'learning_rate': sp_uniform(0.01, 0.3),\n",
        "    'n_estimators': sp_randint(100, 1000),\n",
        "    'min_child_samples': sp_randint(10, 100),\n",
        "    'reg_alpha': sp_uniform(0.0, 1.0),\n",
        "    'reg_lambda': sp_uniform(0.0, 1.0)\n",
        "}"
      ],
      "metadata": {
        "id": "Q5_zmOJd3WHH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "lgbm_rs = get_model(param_grid, x_train, y_train)"
      ],
      "metadata": {
        "id": "R46KATe1l_ln"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "evaluate_model(lgbm_rs, x_test, y_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z2PbmFWbx3KT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "pd.DataFrame(lgbm_rs[\"regressor\"].cv_results_).sort_values(by=\"rank_test_score\", ascending=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6fU__3D9qyw2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip preproc.zip df20.parquet"
      ],
      "metadata": {
        "id": "EeozFouepPcZ",
        "outputId": "83120f60-8b08-423c-b969-f765f1b83da9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: df20.parquet\n",
            "\n",
            "zip error: Nothing to do! (preproc.zip)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valutazione dei modelli\n",
        "andiamo a valutare la performance dei vari modelli prodotti"
      ],
      "metadata": {
        "id": "zSPSjOTH2OYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "rf_gs = no_params_rf"
      ],
      "metadata": {
        "id": "JuZlyss-4a25"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "models_dict = {\n",
        "  \"naive\" : naive_model,\n",
        " \"lasso\" : lasso,\n",
        " \"rf\" : rf_gs,\n",
        " \"xgb\" : xgbm,\n",
        " \"lgbm\" : lgbm_rs,\n",
        "}"
      ],
      "metadata": {
        "id": "Jk37KwIJ3Nvl"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0\n",
        "def save_models(models_dict, folder):\n",
        "  if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "  for model_name, model in models_dict.items():\n",
        "    with open(f\"{folder}/{model_name}.pkl\", \"wb\") as f:\n",
        "      pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "Ym57h61CAGgi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0\n",
        "save_models(models_dict, \"models\")"
      ],
      "metadata": {
        "id": "OO-K9CNGARph"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "def create_evaluation_dict(x, y, models_dict):\n",
        "    results = []\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"Evaluating {model_name}\")\n",
        "        pred = model.predict(x)\n",
        "        R2, RMSPE = evaluate(pred, y)\n",
        "        results.append(\n",
        "          {\"Model\": model_name,\"R2\": R2, \"RMSPE\": RMSPE}\n",
        "        )\n",
        "    #converts results to df\n",
        "    results = pd.DataFrame(results)\n",
        "    results = results.sort_values(by=\"RMSPE\", ascending=True)\n",
        "    return results\n",
        "def plot_results_hist(df):\n",
        "\n",
        "  pos = np.arange(len(df['Model']))\n",
        "  bar_width = 0.2\n",
        "  fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "  # Plotting R2 values\n",
        "  bars1 = ax.barh(pos, df['R2'], bar_width, label='R2', color='orange')\n",
        "\n",
        "  # Plotting RMPSE values\n",
        "  bars2 = ax.barh(pos + bar_width, df['RMSPE'], bar_width, label='RMSPE', color='green')\n",
        "\n",
        "  ax.set_yticks(pos + bar_width / 2)\n",
        "  ax.set_yticklabels(df['Model'])\n",
        "  ax.set_xlabel('Value')\n",
        "  ax.set_title('R2 and RMSPE per modello')\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plot_results_num(df):\n",
        "\n",
        "    # Create a figure and an axis\n",
        "    fig, ax = plt.subplots(figsize=(6, 2))\n",
        "\n",
        "    # Hide the axes\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Create the table\n",
        "    table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bVel2BMQ2Oy7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "results = create_evaluation_dict(x_test, y_test, models_dict)"
      ],
      "metadata": {
        "id": "H_LjGAB64jUu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "plot_results_num(results)"
      ],
      "metadata": {
        "id": "y14z0uOb7E9i"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 0,1\n",
        "plot_results_hist(results)"
      ],
      "metadata": {
        "id": "X7t78fAC5Rwc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Esposizione modello via rest\n",
        "Iniziamo definendo il modello dati che esporremo via api"
      ],
      "metadata": {
        "id": "E7FxsMT97oKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 2\n",
        "import shutil\n",
        "\n",
        "# scarico i modelli pre trainati\n",
        "if not os.path.exists(DOWNLOADED_MODEL_FILE):\n",
        "    print(f\"File does not exist. Downloading...\")\n",
        "    gdown.download(MODEL_URL, DOWNLOADED_MODEL_FILE, quiet=True)\n",
        "else:\n",
        "    print(f\"File already exists\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "41bc0097-36fa-4ded-9fa2-6e9c86cfc440",
        "id": "KaBOp6_Tw8th"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File does not exist. Downloading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 2\n",
        "target_folder = DOWNLOADED_MODEL_FILE.replace(\".zip\", \"\")\n",
        "if os.path.exists(target_folder):\n",
        "    shutil.rmtree(target_folder)\n",
        "shutil.unpack_archive(DOWNLOADED_MODEL_FILE)\n"
      ],
      "metadata": {
        "id": "5Up8ElHTw8tj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 2\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field, validator, root_validator\n",
        "from enum import Enum\n",
        "\n",
        "class BookEntry(BaseModel):\n",
        "    seconds_in_bucket: int\n",
        "    bid_price1: float\n",
        "    ask_price1: float\n",
        "    bid_price2: float\n",
        "    ask_price2: float\n",
        "    bid_size1: float\n",
        "    ask_size1: float\n",
        "    bid_size2: float\n",
        "    ask_size2: float\n",
        "\n",
        "class TradeEntry(BaseModel):\n",
        "    seconds_in_bucket: int\n",
        "    price: float\n",
        "    size: float\n",
        "    order_count: float\n",
        "\n",
        "class Model(str, Enum):\n",
        "    lasso = \"lasso\"\n",
        "    rf = \"rf\"\n",
        "    xgb = \"xgb\"\n",
        "    lgbm = \"lgbm\"\n",
        "\n",
        "class Req(BaseModel):\n",
        "    book: List[BookEntry]\n",
        "    trades: List[TradeEntry]\n",
        "    stock_id: int\n",
        "    model: str"
      ],
      "metadata": {
        "id": "UxkN-iWc7b1S"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HyHKHn1r65k8",
        "outputId": "3d51c0b9-74fe-4661-ad40-a66e7639edd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import nest_asyncio\n",
        "\n",
        "app = FastAPI()\n",
        "models = {}\n",
        "\n",
        "def load_models_(folder):\n",
        "  models_dict = {}\n",
        "  for model_name in os.listdir(folder):\n",
        "      with open(f\"{folder}/{model_name}\", \"rb\") as f:\n",
        "          name = model_name.split(\".\")[0]\n",
        "          if name == \"naive\":\n",
        "            continue\n",
        "          models_dict[name] = pickle.load(f)\n",
        "  return models_dict\n",
        "\n",
        "#at startup load all model in the folder models\n",
        "@app.on_event(\"startup\")\n",
        "def load_models():\n",
        "  global models\n",
        "  models = load_models_(\"models\")\n",
        "  print(\"Loaded models\")\n",
        "  print(models.keys())\n",
        "\n",
        "def to_dict(data):\n",
        "  return  [item.dict() for item in data]\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(request: Req):\n",
        "    global book\n",
        "    model = request.model\n",
        "    model = models[model]\n",
        "    book = pd.DataFrame(to_dict(request.book))\n",
        "    trade = pd.DataFrame(to_dict(request.trades))\n",
        "    #add mock time id\n",
        "    book[\"time_id\"] = 0\n",
        "    trade[\"time_id\"] = 0\n",
        "    df = process(book, trade)\n",
        "    # add a stock_id column\n",
        "    df[\"stock_id\"] = request.stock_id\n",
        "    pred = model.predict(df)[0]\n",
        "    print(pred)\n",
        "    return {\"predicted_volatility\": pred}\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Running\"}\n"
      ],
      "metadata": {
        "id": "lE2iCXNb8sUz",
        "outputId": "4c2b108a-cc89-47ce-e2a8-fe9c3775518c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-3294847d2467>:24: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%mode 2\n",
        "# Per lanciare da colab\n",
        "#!ngrok authtoken XXXX\n",
        "#ngrok_tunnel = ngrok.connect(8000)\n",
        "#print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "fXUM-d4Z7ZJL",
        "outputId": "c73e959f-6d01-4490-fe9b-0b3c5afcc8db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3182]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lasso\n",
            "rf\n",
            "xgb\n",
            "lgbm\n",
            "naive\n",
            "Loaded models\n",
            "dict_keys(['lasso', 'rf', 'xgb', 'lgbm'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [3182]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5Co1mRFjqHSt",
        "MzcooyAOqLKA",
        "2d5AptNcqQVm"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}